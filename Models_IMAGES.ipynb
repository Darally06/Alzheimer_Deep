{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd55b87",
   "metadata": {},
   "source": [
    "# **Modelos 3D para im√°genes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e53aaa1",
   "metadata": {},
   "source": [
    "Notebook dise√±ado para **PROYECTO ALZH** üß†  \n",
    "Permite entrenar y comparar **ResNet18, ResNet34** sobre tus vol√∫menes `.npy` de resonancia magn√©tica.\n",
    "\n",
    "**Caracter√≠sticas principales:**\n",
    "- Vol√∫menes: `(160,192,192)`\n",
    "- Se extraen 10 cortes axiales centrales\n",
    "- Entrada redimensionada a `224√ó224` (para ResNet)\n",
    "- P√©rdida: `BCEWithLogitsLoss` con `pos_weight` (para balanceo)\n",
    "- Evaluaci√≥n: **Balanced Accuracy (BAC)**\n",
    "- Modelos con **Batch Normalization** (ya incluida en ResNet)\n",
    "\n",
    "> ‚ö†Ô∏è Este notebook no entrena autom√°ticamente al abrirlo.  \n",
    "> Primero se hace una *dry run* para verificar que todo funciona.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39a8e94b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.4.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import os, glob, math, random, time, json, torch_directml\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models import resnet18, resnet34, resnet50\n",
    "from torchvision.models import ResNet18_Weights, ResNet34_Weights, ResNet50_Weights\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit, train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "print('PyTorch version', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c959d923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration \n",
    "DATA_DIR = Path(r\"C:\\Users\\Hp\\MACHINE\\MRI\\datos\\procesadas\")\n",
    "LABELS_CSV = r\"C:\\Users\\Hp\\MACHINE\\MRI\\notebooks\\Data\\atributos.csv\"\n",
    "OUTPUT_DIR = \"models_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32b9029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions: skull strip placeholder, intensity normalization, slice extraction\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def skull_strip_placeholder(vol):\n",
    "    \"\"\"\n",
    "    Placeholder: eliminaci√≥n de cr√°neo.\n",
    "    Si el volumen a√∫n tiene cr√°neo, aqu√≠ podr√≠as aplicar un m√©todo m√°s robusto.\n",
    "    Por ahora, solo devuelve el volumen original.\n",
    "    \"\"\"\n",
    "\n",
    "def contrast_normalize_volume(vol, pmin=2, pmax=98):     \n",
    "    vmin = np.percentile(vol, pmin)    \n",
    "    vmax = np.percentile(vol, pmax)    \n",
    "    vol = np.clip(vol, vmin, vmax)    \n",
    "    vol = (vol - vmin) / (vmax - vmin + 1e-9)    \n",
    "    return vol.astype(np.float32)\n",
    "\n",
    "def get_central_slices(vol, n_slices=10):  \n",
    "    \"\"\" Extract n central axial slices from a volume shaped.   \"\"\"\n",
    "    D = vol.shape[0]    \n",
    "    center = D // 2    \n",
    "    half = n_slices // 2   \n",
    "    start = max(0, center - half)    \n",
    "    end = min(D, start + n_slices)\n",
    "    slices = vol[start:end]\n",
    "    if slices.shape[0] < n_slices:  # padding si faltan\n",
    "        pad_before = (n_slices - slices.shape[0]) // 2\n",
    "        pad_after = n_slices - slices.shape[0] - pad_before\n",
    "        slices = np.concatenate([\n",
    "            np.repeat(slices[[0]], pad_before, axis=0),\n",
    "            slices,\n",
    "            np.repeat(slices[[-1]], pad_after, axis=0)\n",
    "        ], axis=0)\n",
    "    return slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2575bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset: loads one volume per item and returns tensor of shape (n_slices, C, H, W)\n",
    "class MRIVolumeDataset(Dataset):\n",
    "    def __init__(self, records, n_slices=10, target_size=(224,224), transform=None, skull_strip=False):\n",
    "        self.records = records        \n",
    "        self.n_slices = n_slices        \n",
    "        self.target_size = target_size        \n",
    "        self.transform = transform        \n",
    "        self.skull_strip = skull_strip    \n",
    "        \n",
    "    def __len__(self):        \n",
    "        return len(self.records)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        rec = self.records[idx]\n",
    "        vol = np.load(rec['path'])  # Volumen (160,192,192)\n",
    "\n",
    "        if self.skull_strip:\n",
    "            vol = skull_strip_placeholder(vol)\n",
    "\n",
    "        vol = contrast_normalize_volume(vol)\n",
    "        slices = get_central_slices(vol, self.n_slices)\n",
    "\n",
    "        imgs = []\n",
    "        for s in slices:\n",
    "            img = Image.fromarray((s * 255).astype(np.uint8))\n",
    "            img = img.resize(self.target_size)\n",
    "            img = img.convert(\"RGB\")\n",
    "\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            else:\n",
    "                img = T.Compose([\n",
    "                    T.ToTensor(),\n",
    "                    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "                ])(img)\n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = torch.stack(imgs, dim=0)  # (n_slices, C, H, W)\n",
    "        label = torch.tensor(rec['label'], dtype=torch.float32)\n",
    "        \n",
    "        return imgs, label, rec['sujeto_id']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cbdec77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetSliceClassifier(nn.Module):\n",
    "    def __init__(self, backbone_name='resnet18', pretrained=False, n_slices=10, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.n_slices = n_slices\n",
    "\n",
    "        if backbone_name == 'resnet18':\n",
    "            base = resnet18(weights=None if not pretrained else ResNet18_Weights.DEFAULT)\n",
    "            feat_dim = base.fc.in_features\n",
    "\n",
    "        elif backbone_name == 'resnet34':\n",
    "            base = resnet34(weights=None if not pretrained else ResNet34_Weights.DEFAULT)\n",
    "            feat_dim = base.fc.in_features\n",
    "        else:\n",
    "            raise ValueError(f\"Modelo no soportado: {backbone_name}\")\n",
    "\n",
    "        self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(feat_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, S, C, H, W = x.shape\n",
    "        x = x.view(B*S, C, H, W)\n",
    "        feats = self.backbone(x)\n",
    "        feats = self.global_pool(feats).view(B, S, -1)\n",
    "        agg = feats.mean(dim=1)\n",
    "        out = self.dropout(agg)\n",
    "        logits = self.classifier(out).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46eb0fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_records(data_dir, labels_csv):\n",
    "    labels_df = pd.read_csv(labels_csv, dtype={'sujeto_id': str})\n",
    "    labels_df = labels_df.dropna(subset=['is_dementia']) # EXCLUIR LOS 28 \n",
    "    labels_df['sujeto_id'] = labels_df['sujeto_id'].str.strip()\n",
    "    label_map = dict(zip(labels_df['sujeto_id'], labels_df['is_dementia']))\n",
    "\n",
    "    records = []\n",
    "    for p in glob.glob(os.path.join(data_dir, \"*.npy\")):\n",
    "        subj = os.path.basename(p)[:10].strip()\n",
    "        if subj in label_map:\n",
    "            records.append({\"path\": p, \"sujeto_id\": subj, \"label\": int(label_map[subj])})\n",
    "    return records\n",
    "\n",
    "\n",
    "def group_stratified_split(records, train_size=0.7, val_size=0.15, test_size=0.15, seed=42):\n",
    "    df = pd.DataFrame(records)\n",
    "    subj_lab = df.groupby(\"sujeto_id\")[\"label\"].agg(lambda x: int(round(x.mean())))\n",
    "    subjects = subj_lab.index.to_list()\n",
    "    y = subj_lab.values\n",
    "\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=train_size, random_state=seed)\n",
    "    train_idx, rest_idx = next(gss.split(subjects, y, groups=subjects))\n",
    "    train_subj = [subjects[i] for i in train_idx]\n",
    "    rest_subj = [subjects[i] for i in rest_idx]\n",
    "\n",
    "    val_prop = val_size / (val_size + test_size)\n",
    "    val_subj, test_subj = train_test_split(rest_subj, test_size=1 - val_prop, random_state=seed, stratify=[subj_lab[s] for s in rest_subj])\n",
    "\n",
    "    def select_by_subject(subj_list):\n",
    "        return [r for r in records if r[\"sujeto_id\"] in subj_list]\n",
    "\n",
    "    return select_by_subject(train_subj), select_by_subject(val_subj), select_by_subject(test_subj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84badd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_weights(records):\n",
    "    labels = [r['label'] for r in records]\n",
    "    cnt0 = sum(l == 0 for l in labels)\n",
    "    cnt1 = sum(l == 1 for l in labels)\n",
    "    pos_weight = torch.tensor([cnt0 / (cnt1 + 1e-9)], dtype=torch.float32)\n",
    "    return pos_weight, cnt0, cnt1\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    ys, preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y, _ in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            probs = torch.sigmoid(model(X)).cpu().numpy()\n",
    "            preds.extend((probs >= 0.5).astype(int))\n",
    "            ys.extend(y.cpu().numpy().astype(int))\n",
    "    return balanced_accuracy_score(ys, preds)\n",
    "\n",
    "\n",
    "def train_one_model(model, train_loader, val_loader, device, pos_weight, lr=1e-4, epochs=10, patience=3):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(device))\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=WEIGHT_DECAY)\n",
    "    \n",
    "    best_bac = 0\n",
    "    best_state = None\n",
    "    patience_counter = 0\n",
    "    history = {\"epoch\": [], \"train_loss\": [], \"val_bac\": []}\n",
    "    time_1 = time.time()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for X, y, _ in train_loader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            opt.zero_grad()\n",
    "            loss = criterion(model(X), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            total_loss += loss.item()\n",
    "        val_bac = evaluate_model(model, val_loader, device)\n",
    "\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        print(f\"Epoch {e+1}/{epochs} | Loss {total_loss/len(train_loader):.4f} | Val BAC {val_bac:.4f} | Tiempo {epoch_time/60:.2f} minutos\")\n",
    "        \n",
    "        # Guardar historia\n",
    "        history[\"epoch\"].append(e + 1)\n",
    "        history[\"train_loss\"].append(avg_loss)\n",
    "        history[\"val_bac\"].append(val_bac)\n",
    "\n",
    "        # --- Early stopping ---\n",
    "        if val_bac > best_bac:\n",
    "            best_bac = val_bac\n",
    "            best_state = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping activado en epoch {e+1}\")\n",
    "                break\n",
    "\n",
    "    total_time = time.time() - time_1\n",
    "    print(f\"Entrenamiento completo en {total_time/60:.2f} minutos\")\n",
    "    model.load_state_dict(best_state)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0741e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_training_history(history, model_name=\"modelo\"):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.plot(history[\"epoch\"], history[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    plt.plot(history[\"epoch\"], history[\"val_bac\"], label=\"Val Balanced Accuracy\", marker=\"s\")\n",
    "    plt.xlabel(\"√âpoca\")\n",
    "    plt.ylabel(\"Valor\")\n",
    "    plt.title(f\"Evoluci√≥n del entrenamiento - {model_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Guardar imagen\n",
    "    filename = f\"models_output/history_{model_name}.png\"\n",
    "    plt.savefig(filename, dpi=300)\n",
    "    plt.show()\n",
    "    print(f\"Gr√°fico guardado en: {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37a3019",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(dry_run=True):\n",
    "    records = build_records(DATA_DIR, LABELS_CSV)\n",
    "    print(f\"Vol√∫menes encontrados: {len(records)}\")\n",
    "    train_recs, val_recs, test_recs = group_stratified_split(records, seed = SEED)\n",
    "    pos_weight, c0, c1 = compute_class_weights(train_recs)\n",
    "    print(f\"Clases: 0={c0}, 1={c1}, pos_weight={pos_weight.item():.2f}\")\n",
    "    print(f\"Bach {BATCH_SIZE}, slices {N_SLICES}, epocas {EPOCHS}\")\n",
    "\n",
    "    transform = T.Compose([\n",
    "        T.RandomHorizontalFlip(0.5),\n",
    "        T.RandomRotation(10),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "    train_ds = MRIVolumeDataset(train_recs, transform=transform)\n",
    "    val_ds = MRIVolumeDataset(val_recs)\n",
    "    test_ds = MRIVolumeDataset(test_recs)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "    if dry_run:\n",
    "        X, y, _ = next(iter(train_loader))\n",
    "        print(\"Ejemplo:\", X.shape, y.shape)\n",
    "        return\n",
    "\n",
    "    results = []\n",
    "    for backbone in ['resnet18','resnet34']:\n",
    "        print(f\"\\n=== Entrenando {backbone} ===\")\n",
    "        model = ResNetSliceClassifier(backbone, n_slices=N_SLICES) # type: ignore\n",
    "        trained, history = train_one_model(model, train_loader, val_loader, DEVICE, pos_weight, lr=LR, epochs=EPOCHS)\n",
    "        plot_training_history(history, model_name=f\"{backbone}_slices{N_SLICES}\")\n",
    "        test_bac = evaluate_model(trained, test_loader, DEVICE)\n",
    "        results.append((backbone, test_bac))\n",
    "        print(f\"Test BAC ({backbone}): {test_bac:.4f}\")\n",
    "\n",
    "         # üîπ guardar modelo\n",
    "        torch.save(trained.state_dict(), f\"{OUTPUT_DIR}/{backbone}_Slices{N_SLICES}_Learning{LR}_model.pth\")\n",
    "        print(f\"Modelo guardado: {OUTPUT_DIR}/{backbone}_Slices{N_SLICES}_Learning{LR}_model.pth\")\n",
    "\n",
    "    print(\"\\nResumen:\")\n",
    "    for b, bac in results:\n",
    "        print(f\"{b}: {bac:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "205adbbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device -> cpu\n"
     ]
    }
   ],
   "source": [
    "# [config] Hiperpar√°metros\n",
    "TARGET_SIZE = (224,224) # resize for ResNet\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 4\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "# Training hyperparams (you can tune later)\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS = 20\n",
    "\n",
    "print('Device ->', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8630e3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vol√∫menes encontrados: 279\n",
      "Clases: 0=74, 1=116, pos_weight=0.64\n",
      "Bach 8, slices 10, epocas 20\n",
      "\n",
      "=== Entrenando resnet18 ===\n",
      "Epoch 1/20 | Loss 0.5035 | Val BAC 0.5000 | Tiempo 2.62 minutos\n",
      "Epoch 2/20 | Loss 0.4416 | Val BAC 0.1860 | Tiempo 6.51 minutos\n",
      "Epoch 3/20 | Loss 0.2967 | Val BAC 0.5581 | Tiempo 9.67 minutos\n"
     ]
    }
   ],
   "source": [
    "# 1. \n",
    "N_SLICES = 10\n",
    "LR = 1e-4\n",
    "main(dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa0d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. \n",
    "N_SLICES = 10\n",
    "LR =  3e-5\n",
    "main(dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c4aa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.\n",
    "N_SLICES = 20\n",
    "LR = 1e-4\n",
    "main(dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55907dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.\n",
    "N_SLICES = 20\n",
    "LR =  3e-5\n",
    "main(dry_run=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10103586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vol√∫menes encontrados: 279\n",
      "Clases: 0=74, 1=116, pos_weight=0.64\n",
      "Bach 8, slices 20\n",
      "\n",
      "=== Entrenando resnet18 ===\n",
      "Epoch 1/15 | Loss 0.5421 | Val BAC 0.5000 | Tiempo 4.53 minutos\n",
      "Epoch 2/15 | Loss 0.4310 | Val BAC 0.2413 | Tiempo 4.40 minutos\n",
      "Epoch 3/15 | Loss 0.3923 | Val BAC 0.5000 | Tiempo 4.38 minutos\n",
      "Epoch 4/15 | Loss 0.2993 | Val BAC 0.3721 | Tiempo 4.63 minutos\n",
      "Early stopping activado en epoch 4\n",
      "Entrenamiento completo en 17.93 minutos\n",
      "Test BAC (resnet18): 0.4444\n",
      "Modelo guardado: models_output/resnet18_best_model.pth\n",
      "\n",
      "=== Entrenando resnet34 ===\n",
      "Epoch 1/15 | Loss 0.5636 | Val BAC 0.5581 | Tiempo 6.91 minutos\n",
      "Epoch 2/15 | Loss 0.4844 | Val BAC 0.3983 | Tiempo 6.87 minutos\n",
      "Epoch 3/15 | Loss 0.4577 | Val BAC 0.5349 | Tiempo 7.11 minutos\n",
      "Epoch 4/15 | Loss 0.3193 | Val BAC 0.5698 | Tiempo 7.05 minutos\n",
      "Epoch 5/15 | Loss 0.2173 | Val BAC 0.6279 | Tiempo 7.03 minutos\n",
      "Epoch 6/15 | Loss 0.2792 | Val BAC 0.7442 | Tiempo 7.00 minutos\n",
      "Epoch 7/15 | Loss 0.1515 | Val BAC 0.5814 | Tiempo 6.63 minutos\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dry run: verifica carga y shapes\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#main(dry_run=True)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Cuando todo funcione, entrena de verdad:\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdry_run\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 33\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(dry_run)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Entrenando \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackbone\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNetSliceClassifier(backbone, n_slices\u001b[38;5;241m=\u001b[39mN_SLICES)\n\u001b[1;32m---> 33\u001b[0m trained \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m test_bac \u001b[38;5;241m=\u001b[39m evaluate_model(trained, test_loader, DEVICE)\n\u001b[0;32m     35\u001b[0m results\u001b[38;5;241m.\u001b[39mappend((backbone, test_bac))\n",
      "Cell \u001b[1;32mIn[8], line 38\u001b[0m, in \u001b[0;36mtrain_one_model\u001b[1;34m(model, train_loader, val_loader, device, pos_weight, lr, epochs, patience)\u001b[0m\n\u001b[0;32m     36\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     37\u001b[0m opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m, y)\n\u001b[0;32m     39\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     40\u001b[0m opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 24\u001b[0m, in \u001b[0;36mResNetSliceClassifier.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m B, S, C, H, W \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mS, C, H, W)\n\u001b[1;32m---> 24\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m feats \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_pool(feats)\u001b[38;5;241m.\u001b[39mview(B, S, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     26\u001b[0m agg \u001b[38;5;241m=\u001b[39m feats\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Hp\\anaconda3\\envs\\MACHINE\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Dry run: verifica carga y shapes\n",
    "#main(dry_run=True)\n",
    "\n",
    "# Cuando todo funcione, entrena de verdad:\n",
    "#main(dry_run=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c62898",
   "metadata": {},
   "source": [
    "````\n",
    "Vol√∫menes encontrados: 279\n",
    "Clases: 0=74, 1=116, pos_weight=0.64\n",
    "\n",
    "=== Entrenando resnet18 ===\n",
    "Epoch 1/15 | Loss 0.5104 | Val BAC 0.5000 | Tiempo 3.12 minutos\n",
    "Epoch 2/15 | Loss 0.4874 | Val BAC 0.5000 | Tiempo 3.14 minutos\n",
    "Epoch 3/15 | Loss 0.3641 | Val BAC 0.5465 | Tiempo 3.14 minutos\n",
    "Epoch 4/15 | Loss 0.3012 | Val BAC 0.5814 | Tiempo 2.69 minutos\n",
    "Epoch 5/15 | Loss 0.3391 | Val BAC 0.4593 | Tiempo 2.75 minutos\n",
    "Epoch 6/15 | Loss 0.3053 | Val BAC 0.4186 | Tiempo 3.08 minutos\n",
    "Epoch 7/15 | Loss 0.2211 | Val BAC 0.6512 | Tiempo 3.05 minutos\n",
    "Epoch 8/15 | Loss 0.1765 | Val BAC 0.6163 | Tiempo 3.12 minutos\n",
    "Epoch 9/15 | Loss 0.2379 | Val BAC 0.6279 | Tiempo 3.23 minutos\n",
    "Epoch 10/15 | Loss 0.1450 | Val BAC 0.5145 | Tiempo 2.86 minutos\n",
    "Early stopping activado en epoch 10\n",
    "Entrenamiento completo en 30.19 minutos\n",
    "Test BAC (resnet18): 0.6528\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a81ab1a",
   "metadata": {},
   "source": [
    "El modelo ResNet18 entrenado desde cero con vol√∫menes de resonancia magn√©tica alcanz√≥ un Balanced Accuracy (BAC) de 0.65 en el conjunto de prueba, mostrando una mejora sustancial frente al desempe√±o anterior (0.51). Esto indica que el modelo ya logra distinguir patrones anat√≥micos asociados a la progresi√≥n de demencia con una precisi√≥n moderada, evitando el sobreajuste gracias al uso de early stopping y a una mayor cantidad de cortes por volumen, que proporcionaron un contexto estructural m√°s completo del cerebro. La reducci√≥n progresiva de la p√©rdida y la estabilidad del BAC durante las √∫ltimas √©pocas evidencian un entrenamiento consistente y un equilibrio adecuado entre sensibilidad y especificidad. En conjunto, estos resultados sugieren que la red est√° capturando caracter√≠sticas relevantes del tejido cerebral y constituye una base s√≥lida para extender el modelo hacia arquitecturas 3D o estrategias de validaci√≥n m√°s robustas, que podr√≠an potenciar a√∫n m√°s su capacidad de generalizaci√≥n."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MACHINE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
